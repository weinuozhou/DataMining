# 数据预处理

预处理的一些通用方法:

```python
get_params([deep]) # 返回模型的参数
set_params(**params) # 设置模型的参数
fit(X[, y]) # 获取预处理需要的参数（如：特征的最大值、最小值等），不同的预处理方法需要的参数不同
transform(X[, copy]) # 执行预处理，返回处理后的样本集
fit_transform(X[, y]) # 获取预处理需要的参数并执行预处理，返回处理后的样本集。
```

预处理的一些通用参数：
* `copy`：布尔值，是否对原数组进行复制，如果为 `False` 则执行原地修改。此时节省空间，但修改了原始数据


## 特征处理

### 二元化

二元化 `Binarizer` 的原型为:

```python
class sklearn.preprocessing.Binarizer(threshold=0.0, copy=True)
```

* `threshold`：浮点数，二值化的阈值，默认为0.0
* `copy`：布尔值，是否对原数组进行复制，默认为True

方法：

* `fit(X[, y])`：获取二值化的阈值
* `transform(X[, copy])`：执行二值化
* `fit_transform(X[, y])`：获取二值化的阈值并执行二值化


### 标准化

### OneHotEncoder

`OneHotEncoder` 的原型为:

```python
class sklearn.preprocessing.OneHotEncoder(n_values='auto', categorical_features='all', dtype=<class 'numpy.float64'>, sparse=True, handle_unknown='error')
```

* `n_values`：整数或字符串，每个特征中唯一值的数量。如果为`auto`，则每个特征中唯一值的数量由输入数据确定
* `categorical_features` ：字符串`all`，或者下标的数组，或者是一个 `mask` ，指定哪些特征需要独热码编码
* `dtype`：指定了独热码编码的数值类型，默认为 `np.float`
* `sparse`：一个布尔值，指定编码结果是否作为稀疏矩阵
* `handle_unknown`：指定转换过程中遇到了未知的 `categorical` 特征时的异常处理策略, 可以为`error` 或 `ignore`，默认为 `error`

属性：

* `n_values_`：每个特征中唯一值的数量
* `active_feature`: 存放转换后的特征中哪些是由独热码编码而来
* `feature_indices_`：每个特征在独热码编码后的索引

方法：

* `fit(X[, y])`：获取每个特征中唯一值的数量
* `transform(X[, y])`：执行独热码编码
* `fit_transform(X[, y])`：获取每个特征中唯一值的数量并执行独热码编码

### 标准化

#### MinMaxScaler

`MinMaxScaler`实现了 `min-max` 标准化, 原型为:

```python
class sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)
```

* `feature_range`：浮点数，元组，指定特征的最小值和最大值，默认为(0, 1)
* `copy`：布尔值，是否对原数组进行复制，默认为True

属性:
* `min_`：每个特征的最小值
* `scale_`：每个特征的缩放因子
* `data_min_`: 给出了每个特征的原始最小值
* `data_max_`: 给出了每个特征的原始最大值
* `data_range_`: 给出了每个特征的原始的范围

方法：
* `fit(X[, y])`：获取每个特征的最小值和最大值
* `transform(X[, copy])`：执行标准化
* `fit_transform(X[, y])`：获取每个特征的最小值和最大值并执行标准化
* `inverse_transform(X[, copy])`：执行逆标准化
* `partial_fit(X[, y])`：部分拟合，用于流式数据

#### MaxAbsScaler

`MaxAbsScaler`实现了 `max-abs` 标准化, 原型为:

```python
class sklearn.preprocessing.MaxAbsScaler(copy=True)
```

* `copy`：布尔值，是否对原数组进行复制，默认为True

#### RobustScaler

`RobustScaler` 实现了 `Robust`标准化, 原型为:

```python
class sklearn.preprocessing.RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)
```

* `with_centering`：布尔值，是否对数据进行中心化处理，默认为True
* `with_scaling`：布尔值，是否对数据进行缩放处理，默认为True
* `quantile_range`：浮点数，元组，指定分位数范围，默认为(25.0, 75.0)
* `copy`：布尔值，是否对原数组进行复制，默认为True

属性：
* `center_`：每个特征的中心值
* `scale_`：每个特征的缩放因子  

方法：
* `fit(X[, y])`：获取每个特征的中心值和缩放因子
* `transform(X[, copy])`：执行标准化
* `fit_transform(X[, y])`：获取每个特征的中心值和缩放因子并执行标准化
* `inverse_transform(X[, copy])`：执行逆标准化
* `partial_fit(X[, y])`：部分拟合，用于流式数据

#### StandardScaler

`StandardScaler` 的原型为:

```python
class sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)
```

* `copy`：布尔值，是否对原数组进行复制，默认为True
* `with_mean`：布尔值，是否对数据进行中心化处理，默认为True
* `with_std`：布尔值，是否对数据进行缩放处理，默认为True

属性：
* `mean_`：给出了原始数据每个特征的均值
* `scale_`：给出了每个特征的缩放倍数的倒数
* `var_`: 给出了每个特征的方差
* `n_samples_seen_`: 给出了已经处理过的样本数量

方法：参考[MinMaxScaler](#MinMaxScaler)

#### QuantileTransformer

`QuantileTransformer`这种方法特别适合处理具有非正态分布和异常值的数据，因为它可以有效地平滑数据分布，使其更接近目标分布, 原型为:

```python
class sklearn.preprocessing.QuantileTransformer(n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=10000, random_state=None, copy=True)
```

* `n_quantiles`：整数，指定分位数数量，默认为1000
* `output_distribution`：字符串，指定输出分布，可以为`uniform` 或 `normal`，默认为`uniform`
* `ignore_implicit_zeros`：布尔值，是否忽略隐式的零，默认为False

属性：
* `quantiles_`：给出了每个特征的分位数
* `inverse_quantiles_`：给出了每个特征的逆分位数
* `n_quantiles_`：给出了每个特征的分位数数量
* `output_distribution_`：给出了每个特征的输出分布

方法：
* `fit(X[, y])`：获取每个特征的分位数
* `transform(X[, copy])`：执行标准化
* `fit_transform(X[, y])`：获取每个特征的分位数并执行标准化
* `inverse_transform(X[, copy])`：执行逆标准化
* `partial_fit(X[, y])`：部分拟合，用于流式数据

#### PowerTransformer

`PowerTransformer`这种方法对于处理具有非正态分布的特征特别有用，尤其是在机器学习建模中，可以提高模型的性能和稳定性, 原型为:

```python
class sklearn.preprocessing.PowerTransformer(method='yeo-johnson', standardize=True, copy=True)
```

* `method`：字符串，指定转换方法
  * `yeo-johnson`：使用 `Yeo-Johnson` 转换, 适合具有正负值的数据
  * `box-cox`：使用 `Box-Cox` 转换, 适合具有正值的数据
* `standardize`：布尔值，是否对数据进行标准化处理，默认为True
* `copy`：布尔值，是否对原数组进行复制，默认为True

属性：
* `lambdas_`：给出了每个特征的转换参数
* `n_features_in_`：输入数据中的特征数量

方法：
* `fit(X[, y])`：获取每个特征的转换参数
* `transform(X[, copy])`：执行标准化
* `fit_transform(X[, y])`：获取每个特征的转换参数并执行标准化
* `inverse_transform(X[, copy])`：执行逆标准化
* `partial_fit(X[, y])`：部分拟合，用于流式数据

### 正则化

`Normalizer` 实现了数据正则化, 原型为：

```python
class sklearn.preprocessing.Normalizer(norm='l2', copy=True)
```

* `norm`：字符串，指定正则化方法
  * `l1`: $L_1$ 范数
  * `l2`: $L_2$ 范数
  * `max`: 最大范数
* `copy`：布尔值，是否对原数组进行复制，默认为True

方法：
* `fit(X[, y])`：获取每个特征的范数
* `transform(X[, copy])`：执行标准化
* `fit_transform(X[, y])`：获取每个特征的范数并执行标准化

## 特征选择

### 过滤式特征选取

#### VarianceThreshold

`VarianceThreshold` 用于剔除方差很小的特征, 其原型为：

```python
class sklearn.feature_selection.VarianceThreshold(threshold=0.0)
```

* `threshold`：浮点数，指定方差阈值，低于该阈值的特征将被剔除

属性：
* `variances_`：给出了每个特征的方差

方法：
* `fit(X[, y])`：学习每个特征的方差
* `transform(X[, copy])`：执行特征选择
* `fit_transform(X[, y])`：获取每个特征的方差并执行特征选择
* `get__support([indices])`：获取被保留的特征的掩码
* `inverse_transform(X[, copy])`：执行逆特征选择

#### SelectKBest

`SelectKBest` 用于选取得分最高的k个特征, 其原型为：

```python
class sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, k=10)
```

* `score_func`：函数，指定得分函数，默认为`f_classif`，即方差分析F值
* `k`：整数，指定选取的特征数量，默认为10

属性：
* `scores_`：给出了每个特征的得分
* `pvalues_`：给出了每个特征的p值

方法： 参考[VarianceThreshold](#variancethreshold)

#### SelectPercentile

`SelectPercentile` 用于选取得分最高的百分比的特征, 其原型为：

```python
class sklearn.feature_selection.SelectPercentile(score_func=<function f_classif>, percentile=10)
```

* `score_func`：函数，指定得分函数，默认为`f_classif`，即方差分析F值
* `percentile`：整数，指定选取的特征百分比，默认为10

属性：参考[SelectKBest](SelectKBest)

方法：参考[VarianceThreshold](#variancethreshold)

### 包裹式特征选取

#### RFE

`RFE` 使用递归特征消除方法进行特征选择, 其原型为：

```python
class sklearn.feature_selection.RFE(estimator, n_features_to_select=None, step=1, verbose=0)
```

* `estimator`：模型，指定用于特征选择的模型
* `n_features_to_select`：整数，指定选取的特征数量
* `step`：整数，指定每次递归剔除的特征数量，默认为1
* `verbose`：整数，指定输出日志的级别，默认为0

属性：
* `support_`：给出了被保留的特征的掩码
* `ranking_`：给出了每个特征的排名
* `n_features`：给出了被保留的特征数量
* `estimators`：外部提供的学习器 

方法：
* `fit(X[, y])`：学习每个特征的得分
* `transform(X[, copy])`：执行特征选择
* `fit_transform(X[, y])`：获取每个特征的得分并执行特征选择
* `get_support([indices])`：获取被保留的特征的掩码
* `inverse_transform(X[, copy])`：执行逆特征选择
* `score(X, y)`: 计算得分

#### RFECV

`RFECV` 使用递归特征消除方法进行特征选择，并使用交叉验证来选择最佳特征数量, 其原型为：

```python
class sklearn.feature_selection.RFECV(estimator, step=1, cv=None, scoring=None, verbose=0)
```

* `estimator`：模型，指定用于特征选择的模型
* `step`：整数，指定每次递归剔除的特征数量，默认为1
* `cv`：交叉验证生成器，指定交叉验证方法
* `scoring`：字符串或函数，指定评分函数

属性：
* `support_`：给出了被保留的特征的掩码
* `ranking_`：给出了每个特征的排名
* `n_features`：给出了被保留的特征数量
* `estimators_`：外部提供的学习器
* `grid_scores_`：给出了每个特征数量的交叉验证得分

方法：参考[RFE](#RFE)

### 嵌入式特征选取

#### L1-based feature selection

`L1-based feature selection` 是一种特征选择方法，利用 $L_1$ 正则化来选择最重要的特征。它通常与线性模型（如线性回归和逻辑回归）结合使用，通过引入 $L_1$ 正则化项（也称为 $L-1$ 惩罚）来促进特征的稀疏性，即使得一些特征的系数变为零，从而实现特征选择, 其原型为：

```python
class sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
```

* `alpha`：浮点数，指定正则化系数
* `fit_intercept`：布尔值，是否计算截距，默认为True
* `normalize`：布尔值，是否对数据进行标准化处理，默认为False
* `precompute`：布尔值，是否使用预计算矩阵，默认为False
* `copy_X`：布尔值，是否对原数组进行复制，默认为True
* `max_iter`：整数，指定最大迭代次数，默认为1000
* `tol`：浮点数，指定收敛阈值，默认为0.0001
* `warm_start`：布尔值，是否使用热启动，默认为False
* `positive`：布尔值，是否强制系数为正，默认为False
* `random_state`：整数或随机数生成器，指定随机种子
* `selection`：字符串，指定特征选择方法，默认为`cyclic`

#### Ridge 回归

`Ridge 回归` 是一种线性回归模型，通过引入 $L_2$ 正则化项（也称为 $L-2$ 惩罚）来防止过拟合。它通常用于特征选择，通过调整正则化系数，可以控制模型的复杂度和特征选择的效果, 其原型为：

```python
class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)
```

* `alpha`：浮点数，指定正则化系数
* `fit_intercept`：布尔值，是否计算截距，默认为True
* `normalize`：布尔值，是否对数据进行标准化处理，默认为False
* `copy_X`：布尔值，是否对原数组进行复制，默认为True
* `max_iter`：整数，指定最大迭代次数，默认为None
* `tol`：浮点数，指定收敛阈值，默认为0.001
* `solver`：字符串，指定求解器，默认为`auto`

#### ElasticNet 回归

`ElasticNet 回归` 是一种线性回归模型，结合了 $L_1$ 和 $L_2$ 正则化项，可以同时进行特征选择和防止过拟合。它通常用于特征选择，通过调整正则化系数，可以控制模型的复杂度和特征选择的效果, 其原型为：

```python
class sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, *, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
```

* `alpha`：浮点数，指定正则化系数
* `l1_ratio`：浮点数，指定 $L_1$ 和 $L_2$ 正则化项的权重，默认为0.5
* `fit_intercept`：布尔值，是否计算截距，默认为True
* `normalize`：布尔值，是否对数据进行标准化处理，默认为False
* `precompute`：布尔值，是否使用预计算矩阵，默认为False
* `max_iter`：整数，指定最大迭代次数，默认为1000
* `copy_X`：布尔值，是否对原数组进行复制，默认为True
* `tol`：浮点数，指定收敛阈值，默认为0.0001

#### 特征选择器（Feature Selector）

特征选择器是一些综合模型的工具，通过训练模型来评估和选择特征。常见的特征选择器包括**基于线性模型**的特征选择和**基于树模型**的特征选择

`SelectFromModel` 用于根据模型的特征重要性进行特征选择, 其原型为：

```python
class sklearn.feature_selection.SelectFromModel(estimator=None, threshold=None, prefit=False, max_features=None, norm_order=2, mode='mean', clone_estimator=True)
```

* `estimator`：模型，指定用于特征选择的模型
* `threshold`：浮点数或字符串，指定特征重要性阈值，低于该阈值的特征将被剔除
* `prefit`：布尔值，是否已经训练了模型，默认为False

属性：
* `threshold_`：浮点数，特征重要性阈值

## 字典学习

字典学习（Dictionary Learning）是一种用于数据表示和特征学习的技术，旨在通过学习一个字典来稀疏地表示数据。其核心思想是找到一个“字典”矩阵，使得数据可以通过这个字典的线性组合来有效地表示

### MiniBatchDictionaryLearning

`MiniBatchDictionaryLearning` 是一种用于大规模数据的字典学习算法，它通过使用小批量数据来更新字典，从而实现高效的字典学习。其原型为：

```python
class sklearn.decomposition.MiniBatchDictionaryLearning(n_components=None, alpha=1.0, n_iter=1000, fit_algorithm='lars', transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=None, batch_size=100, shuffle=True, verbose=False, split_sign=False, random_state=None, positive_code=False, positive_dict=False)
```

* `n_components`：整数，指定字典的维度
* `alpha`：浮点数，指定正则化系数
* `n_iter`：整数，指定最大迭代次数
* `fit_algorithm`：字符串，指定字典学习的算法，默认为`lars`
* `transform_algorithm`：字符串，指定特征选择的算法，默认为`omp`
* `transform_n_nonzero_coefs`：整数，指定特征选择的非零系数个数
* `transform_alpha`：浮点数，指定特征选择的正则化系数
* `n_jobs`：整数，指定并行计算的核数
* `batch_size`：整数，指定小批量数据的大小
* `shuffle`：布尔值，是否对小批量数据进行打乱，默认为True
* `verbose`：布尔值，是否输出训练过程中的信息，默认为False
* `split_sign`：布尔值，是否将字典的系数分为正负两部分，默认为False
* `random_state`：整数或随机数生成器，指定随机数种子

属性:
* `components_`：数组，字典矩阵
* `error_`：浮点数，训练过程中的误差
* `n_iter_`：整数，实际迭代次数

### SparseCoder

`SparseCoder` 是一种用于稀疏编码的工具，它可以将数据表示为字典的稀疏线性组合。其原型为：

```python
class sklearn.decomposition.SparseCoder(dictionary=None, transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, transform_algorithm='omp', n_jobs=None, positive_code=False)
```

* `dictionary`：数组，指定字典矩阵
* `transform_n_nonzero_coefs`：整数，指定特征选择的非零系数个数
* `transform_alpha`：浮点数，指定特征选择的正则化系数
* `split_sign`：布尔值，是否将字典的系数分为正负两部分，默认为False
* `transform_algorithm`：字符串，指定特征选择的算法，默认为`omp`
* `n_jobs`：整数，指定并行计算的核数
* `positive_code`：布尔值，是否将字典的系数限制为非负值，默认为False

### DictionaryLearning

`DictionaryLearning` 是一种用于字典学习的算法，它通过最小化数据与字典的稀疏线性组合之间的误差来学习字典。其原型为：

```python
class sklearn.decomposition.DictionaryLearning(n_components=None, alpha=1.0, max_iter=1000, tol=1e-08, fit_algorithm='lars', transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=None, verbose=False, split_sign=False, random_state=None, positive_code=False, positive_dict=False)
```

* `n_components`：整数，指定字典的维度
* `alpha`：浮点数，指定正则化系数
* `max_iter`：整数，指定最大迭代次数
* `tol`：浮点数，指定收敛的容忍度
* `fit_algorithm`：字符串，指定字典学习的算法，默认为`lars`
* `transform_algorithm`：字符串，指定特征选择的算法，默认为`omp`
* `transform_n_nonzero_coefs`：整数，指定特征选择的非零系数个数

属性：
* `components_`：数组，字典矩阵
* `error_`：浮点数，训练过程中的误差
* `n_iter_`：整数，实际迭代次数

## PipeLine

`PipeLine` 的流程通常是:
1. 通过一组特征处理 `estimator` 来对特征进行处理（如标准化、正则化
2. 通过一组特征提取 `estimator` 来提取特征
3. 通过一个模型预测 `estimator` 来学习模型，并执行预测

Pipeline将多个 `estimator` 组成流水线，其原型为：
```python
class sklearn.pipeline.Pipeline(steps, memory=None, verbose=False)
```

* `steps`：列表，包含多个 `estimator`，每个 `estimator` 由一个字符串和一个 `estimator` 对象组成
* `memory`：字符串或字典，指定缓存目录
* `verbose`：布尔值，是否输出训练过程中的信息，默认为False

属性:
* `named_steps_`：字典，包含每个 `estimator` 的名称和 `estimator` 对象

方法：
* `fit(X, y=None, **fit_params)`：训练模型
* `transform(X)`：转换数据
* `predict(X)`：预测数据
* `predict_proba(X)`：预测概率
* `score(X, y, sample_weight=None)`：评估模型




