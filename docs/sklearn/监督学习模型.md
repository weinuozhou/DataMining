# 监督学习模型

模型的通用方法：
* `get_params()`：获取模型参数
* `set_params()`：设置模型参数
* `fit()`：训练模型
* `predict()`：预测
* `score()`：评估模型
  * 对于分类模型，其评估的是 `accuracy` ；对于回归模型，其评估的是 $R_2$ 

模型的通用参数:
* `random_state`：随机种子
* `n_jobs`：并行计算
* `verbose`：输出训练过程
* `max_iter`: 最大迭代次数

## 线性模型

线性模型的通用参数为：
* `fit_intercept`：是否计算截距
* `fit_scaling`: 用于缩放截距项的正则化项的影响
* `tol`: 指定判断迭代收敛与否的阈值

### LinearRegression

`LinearRegression` 是线性回归模型，它的原型为：

```python
class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False)
```

* `fit_intercept`：是否计算截距
* `normalize`：是否对输入数据进行标准化处理
* `copy_X`：是否复制输入数据
* `n_jobs`：并行计算
* `positive`：是否使用正则化

模型属性：
* `coef_`: 回归系数
* `intercept_`: 截距

### Ridge

`Ridge` 类实现了岭回归模型。其原型为：

```python
class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)
```

* `alpha`：正则化系数
* `fit_intercept`：是否计算截距
* `normalize`：是否对输入数据进行标准化处理
* `copy_X`：是否复制输入数据
* `max_iter`：最大迭代次数
* `tol`：指定判断迭代收敛与否的阈值
* `solver`：求解器
  * `auto`：自动选择
  * `svd`：奇异值分解
  * `cholesky`：使用 Cholesky 分解
  * `lsqr`：使用最小平方法
  * `sparse_cg`：使用稀疏共轭梯度法
  * `sag`：使用随机平均梯度下降法
  * `saga`：使用随机平均梯度下降法
* `random_state`：随机种子

模型属性：
* `coef_`: 回归系数
* `intercept_`: 截距
* `n_iter`：实际迭代次数

### Lasso

`Lasso` 类实现了 Lasso 回归模型。其原型为：

```python
class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.001, warm_start=False, positive=False, random_state=None, selection='cyclic')
```

* `alpha`：正则化系数
* `fit_intercept`：是否计算截距
* `normalize`：是否对输入数据进行标准化处理
* `precompute`：是否预计算
* `selection`: 指定了当每轮迭代的时候，选择权重向量的哪个分量来更新
  * `cyclic`：循环选择
  * `random`：随机选择
* `copy_X`：是否复制输入数据
* `max_iter`：最大迭代次数
* `tol`：指定判断迭代收敛与否的阈值
* `warm_start`：是否使用热启动
* `positive`：是否使用正则化
* `random_state`：随机种子

模型属性：
* `coef_`: 回归系数
* `intercept_`: 截距
* `n_iter_`：实际迭代次数

### ElasticNet

ElasticNet类实现了ElasticNet回归模型。其原型为：

```python
class sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.001, warm_start=False, positive=False, random_state=None, selection='cyclic')
```

* `alpha`：正则化系数
* `l1_ratio`：L1 和 L2 正则化项的权重
* `fit_intercept`：是否计算截距
* `normalize`：是否对输入数据进行标准化处理
* `precompute`：是否预计算
* `selection`: 指定了当每轮迭代的时候，选择权重向量的哪个分量来更新
  * `cyclic`：循环选择
  * `random`：随机选择
* `copy_X`：是否复制输入数据
* `max_iter`：最大迭代次数
* `tol`：指定判断迭代收敛与否的阈值
* `warm_start`：是否使用热启动
* `positive`：是否使用正则化
* `random_state`：随机种子

模型属性：
* `coef_`: 回归系数
* `intercept_`: 截距
* `n_iter_`：实际迭代次数

### LogisticRegression

`LogisticRegression` 类实现了逻辑回归模型。其原型为：

```python
class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)
```

* `penalty`：正则化项
  * `l1`：$L_1$ 正则化
  * `l2`：$L_2$ 正则化
* `dual`：是否使用对偶形式
* `tol`：指定判断迭代收敛与否的阈值
* `C`：正则化系数
* `fit_intercept`：是否计算截距
* `intercept_scaling`：当 `intercept` 为 `True` 时，该参数可缩放截距
* `class_weight`：类别权重
* `random_state`：随机种子
* `solver`：指定优化算法
  * `liblinear`：使用 `liblinear` 库
  * `lbfgs`：使用 `L-BFGS` 算法

### LinearDiscriminantAnalysis

`LinearDiscriminantAnalysis` 类实现了线性判别分析模型。其原型为：

```python
class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)
```

* `solver`：指定求解算法
  * `svd`：使用奇异值分解
  * `lsqr`：使用最小平方法
  * `eigen`：使用特征值分解
* `shrinkage`：指定收缩系数
* `priors`：指定类别权重
* `n_components`：指定保留的主成分数量
* `store_covariance`：是否存储协方差矩阵
* `tol`：指定判断迭代收敛与否的阈值

模型属性：
* `coef_`: 回归系数
* `intercept_`: 截距
* `n_iter_`：实际迭代次数
* `mean_`: 依次给出了每个类别的均值向量
* `xbar_`: 给出了整体样本的均值向量

## 支持向量机

### LinearSVC

`LinearSVC` 是根据 `liblinear` 实现的，它可以用于二类分类，也可以用于多类分类问题（此时是根据`one-vs-rest` 原则来分类）, 其原型为：

```python
class sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)
```

* `penalty`：正则化项
  * `l1`：$L_1$ 正则化
  * `l2`：$L_2$ 正则化
* `loss`: 损失函数
  * `squared_hinge`：平方铰链损失函数
  * `hinge`: 标准损失函数
* `dual`：是否使用对偶形式
* `tol`：指定判断迭代收敛与否的阈值
* `C`：正则化系数
* `multi_class`：指定多类分类策略
  * `ovr`：one-vs-rest
  * `crammer_singer`：Crammer-Singer
* `fit_intercept`：是否计算截距
* `intercept_scaling`：当 `intercept` 为 `True` 时，该参数可缩放截距
* `class_weight`：类别权重
* `verbose`：是否输出详细信息
* `random_state`：随机种子
* `max_iter`：最大迭代次数

模型属性:
* `coef_`: 回归系数
* `intercept_`: 截距

### SVC

`SVC` 是根据 `libsvm` 实现的，它可以用于二类分类，也可以用于多类分类问题（此时是根据`one-vs-rest` 原则来分类）, 其原型为：

```python
class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)
```

* `C`：正则化系数
* `kernel`：核函数
  * `linear`：线性核函数
  * `poly`：多项式核函数
  * `rbf`：高斯核函数
  * `sigmoid`：Sigmoid核函数
* `degree`：多项式核函数的阶数
* `gamma`：核函数系数
* `coef0`：核函数中的常数项
* `shrinking`：是否使用收缩
* `probability`：是否计算概率
* `tol`：指定判断迭代收敛与否的阈值
* `cache_size`：指定缓存大小
* `class_weight`：类别权重
* `verbose`：是否输出详细信息
* `max_iter`：最大迭代次数
* `decision_function_shape`：决策函数形状
  * `ovr`：one-vs-rest
  * `ovo`：one-vs-one
* `random_state`：随机种子

模型参数：
* `support_`: 给出了支持向量的下标
* `support_vectors`: 支持向量
* `n_support_`: 每个类别的支持向量数量
* `dual_coef_`: 对偶系数
* `intercept_`: 截距
* `coef_`: 回归系数

### NuSVC

`Nu-Support Vector Classificatio` 与 `SVC` 相似，但是用一个参数来控制了支持向量的个数。它是基于 `libsvm` 来实现的

```python
class sklearn.svm.NuSVC(nu=0.5, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)
```

* `nu`：它控制训练误差与支持向量的比值，间接控制了支持向量的个数

### LinearSVR

`LinearSVR` 是根据 `liblinear` 实现的, 其原型为：

```python
class sklearn.svm.LinearSVR(epsilon=0.1, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)
```

* `epsilon`：误差项
* `loss`：损失函数
  * `epsilon_insensitive`：epsilon-不敏感损失函数
  * `squared_epsilon_insensitive`：平方epsilon-不敏感损失函数

### SVR

支持向量回归 `SVR`:

```python
class sklearn.svm.SVR(kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)
```

* `epsilon`：误差项
* `C`：正则化系数
* `kernel`：核函数
  * `linear`：线性核函数
  * `poly`：多项式核函数
  * `rbf`：高斯核函数
  * `sigmoid`：Sigmoid核函数
* `degree`：多项式核函数的阶数
* `gamma`：核函数系数
* `coef0`：核函数中的常数项
* `shrinking`：是否使用收缩
* `cache_size`：指定缓存大小

### NuSVR

支持向量回归 `NuSVR`:

```python
class sklearn.svm.NuSVR(nu=0.5, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)
```

* `nu`：它控制训练误差与支持向量的比值，间接控制了支持向量的个数

### OneClassSVM

`OneClassSVM` 是根据 `libsvm` 实现的，其原型为：

```python
class sklearn.svm.OneClassSVM(kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1)
```

## 贝叶斯模型

在 `scikit-learn` 中有多种不同的朴素贝叶斯分类器。他们的区别就在于它们假设了不同的概率分布

### 高斯朴素贝叶斯

`GaussianNB` 是基于高斯分布的朴素贝叶斯分类器。其原型为：

```python
class sklearn.naive_bayes.GaussianNB(priors=None)
```

* `priors`：先验概率

模型属性：
* `class_prior_` ：每个类别的概率 
* `class_count_` ：每个类别包含的训练样本数量
* `theta_` ：每个特征的均值
* `sigma_` ：每个特征的标准差

### MultinomialNB

`MultinomialNB` 的原型为:

```python
class sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)
```

* `alpha`：平滑参数
* `fit_prior`：是否要学习类的先验概率
* `class_prior`：类的先验概率

### BernoulliNB

`BernoulliNB` 的原型为：

```python
class sklearn.naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)
```

* `alpha`：平滑参数
* `binarize`：二值化阈值
* `fit_prior`：是否要学习类的先验概率
* `class_prior`：类的先验概率

## 决策树模型

### DecisionTreeRegressor

`DecisionTreeRegressor` 是回归决策树，其原型为:

```python
class sklearn.tree.DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0)
```

* `criterion`：衡量分裂质量的函数
  * `mse`：均方误差
  * `friedman_mse`：改进的均方误差
  * `mae`：绝对误差
* `splitter`：分裂策略
  * `best`：最佳分裂
  * `random`：随机分裂
* `max_depth`：树的最大深度
* `min_samples_split`：分裂所需的最小样本数
* `min_samples_leaf`：叶子节点所需的最小样本数
* `min_weight_fraction_leaf`：叶子节点所需的最小样本权重比例
* `max_features`：寻找最佳分裂时考虑的特征数量
* `random_state`：随机数种子
* `max_leaf_nodes`：最大叶子节点数量
* `min_impurity_decrease`：分裂所需的最小不纯度减少量
* `min_impurity_split`：分裂所需的最小不纯度
* `presort`：是否预先排序
* `ccp_alpha`：剪枝参数

模型参数：
* `feature_importances_`：特征重要性
* `max_features_`：实际使用的特征数量
* `n_features_`：特征数量
* `n_outputs_`：输出数量
* `tree_`: 底层的决策树

### DecisionTreeClassifier

`DecisionTreeClassifier` 是分类决策树，其原型为：

```python
class sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)
```

* `criterion`：衡量分裂质量的函数
* `splitter`：分裂策略
* `max_depth`：树的最大深度
* `min_samples_split`：分裂所需的最小样本数
* `min_samples_leaf`：叶子节点所需的最小样本数
* `min_weight_fraction_leaf`：叶子节点所需的最小样本权重比例
* `max_features`：寻找最佳分裂时考虑的特征数量
* `random_state`：随机数种子
* `max_leaf_nodes`：最大叶子节点数量
* `min_impurity_decrease`：分裂所需的最小不纯度减少量
* `min_impurity_split`：分裂所需的最小不纯度
* `class_weight`：类别权重
* `presort`：是否预先排序
* `ccp_alpha`：剪枝参数

模型参数：
* `classes_`: 类别
* `n_classes_`: 分类的数量

## KNN

### KNeighborsClassifier

`KNeighborsClassifier` 是 `knn` 分类模型, 其原型为:

```python
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)
```

* `n_neighbors`：邻居数量
* `weights`：权重函数
  * `uniform`：均匀权重
  * `distance`：距离权重
* `algorithm`：算法
  * `ball_tree`：球树
  * `kd_tree`：k-d树
  * `brute`：暴力搜索
  * `auto`：自动选择
* `leaf_size`：叶子大小
* `metric`：距离度量
* `metric_params`：距离度量参数
* `n_jobs`：并行计算

### KNeighborsRegressor

`KNeighborsRegressor` 是 `knn` 回归模型，其原型为：

```python
class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)
```

## AdaBoost

### AdaBoostClassifier

`AdaBoostClassifier` 是 AdaBoost 分类模型，其原型为：

```python
class sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)
```

* `base_estimator`：基分类器
* `n_estimators`：基分类器数量
* `learning_rate`：学习率
* `algorithm`：算法
  * `SAMME`：
  * `SAMME.R`：
* `random_state`：随机数种子

模型属性：
* `estimators_`：基分类器
* `classes_`：类别
* `n_classes_`：分类数量
* `estimator_weights_`：基分类器权重
* `estimator_errors_`：基分类器错误率

### AdaBoostRegressor

`AdaBoostRegressor` 是 AdaBoost 回归模型，其原型为：

```python
class sklearn.ensemble.AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)
```

* `base_estimator`：基回归器
* `n_estimators`：基回归器数量
* `learning_rate`：学习率
* `loss`：损失函数
  * `linear`：线性损失
  * `square`：平方损失
  * `exponential`：指数损失
* `random_state`：随机数种子

模型属性：
* `estimators_`：基回归器
* `estimator_weights_`：基回归器权重
* `estimator_errors_`：基回归器错误率

## 梯度提升树

### GradientBoostingClassifier

`GradientBoostingClassifier` 是梯度提升树分类模型，其原型为：

```python
class sklearn.ensemble.GradientBoostingClassifier(loss='deviance', learning_rate=0.1,
n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1,
min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None, 
max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')
```

* `loss`：损失函数
  * `deviance`：对数损失
  * `exponential`：指数损失
* `learning_rate`：学习率
* `n_estimators`：基分类器数量
* `subsample`：子采样比例
* `min_samples_split`：最小分裂样本数
* `min_samples_leaf`：最小叶子样本数
* `min_weight_fraction_leaf`：最小叶子权重比例
* `max_depth`：最大深度
* `init`：初始化函数
* `random_state`：随机数种子

模型属性：
* `feature_importances_`：每个特征的重要性
* `oob_improvement_`：给出训练过程中，每增加一个基础决策树，在测试集上损失函数的改善情况
* `train_score_`：给出训练过程中，每增加一个基础决策树，在训练集上的损失函数的值
* `init`：初始预测使用的分类器
* `estimators_` ：所有训练过的基础决策树

### GradientBoostingRegressor

`GradientBoostingRegressor` 是梯度提升树回归模型，其原型为：

```python
class sklearn.ensemble.GradientBoostingRegressor(loss='ls', learning_rate=0.1,
n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1,
min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None,
max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, 
presort='auto')
```

* `loss`：损失函数
  * `ls`：平方损失
  * `lad`：绝对损失
  * `huber`：Huber损失
  * `quantile`：分位数损失
* `learning_rate`：学习率
* `n_estimators`：基回归器数量
* `subsample`：子采样比例
* `min_samples_split`：最小分裂样本数
* `min_samples_leaf`：最小叶子样本数
* `min_weight_fraction_leaf`：最小叶子权重比例
* `max_depth`：最大深度

模型属性：
* `feature_importances_`：每个特征的重要性
* `oob_improvement_`：给出训练过程中，每增加一个基础决策树，在测试集上损失函数的改善情况
* `train_score_`：给出训练过程中，每增加一个基础决策树，在训练集上的损失函数的值
* `init`：初始预测使用的分类器
* `estimators_` ：所有训练过的基础决策树

## 随机森林

### RandomForestClassifier

`RadomForestClassifier` 是随机森林分类模型，其原型为：

```python
class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini',
max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,
min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None,
random_state=None, verbose=0, warm_start=False, class_weight=None)
```

* `n_estimators`：基分类器数量

### RandomForestRegressor

`RandomForestRegressor` 是随机森林回归模型，其原型为：

```python
class sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion='mse',
max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,
min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None,
random_state=None, verbose=0, warm_start=False)
```

* `n_estimators`：基回归器数量