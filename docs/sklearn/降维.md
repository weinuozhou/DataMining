# 降维

降维（Dimensionality Reduction）是数据预处理中的一个重要步骤，旨在减少数据集中的特征数量，同时尽可能保留原始数据的主要信息。这可以帮助提高计算效率、减少存储需求，并改善某些模型的性能。降维方法主要分为两类：**线性降维**和**非线性降维**

## PCA

### PCA

通过找出数据的主成分（即方差最大的方向）来进行降维, `PCA` 是最常用的线性降维技术,其原型为:

```python
class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)
```
* `n_components`：指定降维后的维度，如果为 `None`，则默认保留所有维度
* `copy`：是否在降维过程中复制数据，默认为 `True`
* `whiten`：是否对降维后的数据进行白化处理，默认为 `False`

属性：
* `components_`：降维后的主成分矩阵
* `explained_variance_`：每个主成分的方差
* `explained_variance_ratio_`：每个主成分的方差占总方差的比率
* `mean_`：原始数据的均值向量
* `n_components_`：降维后的维度

方法：
* `fit(X)`：计算主成分
* `transform(X)`：将数据降维
* `inverse_transform(X)`：将降维后的数据还原到原始维度

将`iris`数据集降到二维的结果为:

<div style="text-align: center;"><img alt='202407231155501' src='https://cdn.jsdelivr.net/gh/weno861/image@main/img/202407231155501.png' > </div>

### IncrementalPCA

`scikit-learn` 中的 `IncrementalPCA` 类也实现了 `PCA` 模型。它适用于超大规模数据，可以将数据分批加载进内存, 其原型为：

```python
class sklearn.decomposition.IncrementalPCA(n_components=None, whiten=False, copy=True, batch_size=100)
```

* `batch_size`：每次加载的数据量，默认为 `100`

### RandomizedPCA

`scikit-learn` 中的 `RandomizedPCA` 类也实现了 `PCA` 模型。它使用随机化方法来计算主成分，适用于大规模数据，其原型为：

```python
class sklearn.decomposition.RandomizedPCA(n_components=None, whiten=False, copy=True)
```

* `n_components`：指定降维后的维度，如果为 `None`，则默认保留所有维度
* `whiten`：是否对降维后的数据进行白化处理，默认为 `False`

### KernelPCA

`KernelPCA` 是 `scikit-learn` 实现的核化 `PCA` 模型，其原型为：

```python
class sklearn.decomposition.KernelPCA(n_components=None, kernel='linear', gamma=None, degree=3, coef0=1, alpha=1.0, fit_inverse_transform=False, eigen_solver='auto', tol=0, max_iter=None, remove_zero_eig=False, random_state=None)
```

* `kernel`：核函数，默认为 `linear`，可选 `linear`、`poly`、`rbf`、`sigmoid` 等
* `gamma`：核函数的参数，默认为 `None`，对于 `rbf`、`poly` 和 `sigmoid` 核函数，需要指定 `gamma` 参数
* `degree`：多项式核函数的次数，默认为 `3`
* `coef0`：多项式核函数的常数项，默认为 `1`
* `alpha`：正则化参数，默认为 `1.0`
* `fit_inverse_transform`：是否拟合逆变换，默认为 `False`

属性：
* `lambdas_`: 核化矩阵的特征值
* `alphas_`: 核化矩阵的特征向量
* `dual_coef_`: 逆转换矩阵

## MDS

多维尺度分析（MDS, Multidimensional Scaling）是一种非线性降维方法，旨在在低维空间中保留数据点之间的距离关系，使得原始高维数据的相似性在低维空间中得到尽可能真实的表示,其原型为:

```python
class sklearn.manifold.MDS(n_components=2, metric=True, n_init=4, max_iter=300, verbose=0, eps=0.001, n_jobs=None, random_state=None, dissimilarity='euclidean')
```

* `n_components`：指定降维后的维度，默认为 `2`
* `metric`：是否使用度量距离，默认为 `True`
* `n_init`：初始化次数，默认为 `4`
* `max_iter`：最大迭代次数，默认为 `300`
* `verbose`：是否输出日志信息，默认为 `0`
* `eps`：停止迭代的阈值，默认为 `0.001`
* `n_jobs`：并行计算使用的 CPU 核数，默认为 `None`

属性：
* `embedding_`：降维后的数据矩阵
* `stress_`：应力值，表示降维后的数据与原始数据之间的差异

## Isomap

等距映射（Isomap, Isometric Mapping）是一种基于流形学习的非线性降维方法。它旨在保留高维数据的全局几何结构，通过计算数据点在流形上的最短路径距离，然后在低维空间中嵌入这些距离。Isomap 可以揭示数据的内在几何结构，对于处理具有复杂非线性关系的数据特别有效, 其原型为：

```python
class sklearn.manifold.Isomap(n_neighbors=5, n_components=2, eigen_solver='auto', tol=0, max_iter=None, path_method='auto', neighbors_algorithm='auto', n_jobs=None)
```

* `n_neighbors`：指定每个数据点的邻居数量，默认为 `5`
* `n_components`：指定降维后的维度，默认为 `2`
* `eigen_solver`：特征值求解方法，默认为 `auto`
* `tol`：停止迭代的阈值，默认为 `0`
* `path_method`：路径计算方法
  * `auto`：自动选择
  * `FW`：Floyd-Warshall 算法
  * `D`：Dijkstra 算法
* `neighbors_algorithm`：邻居搜索算法，默认为 `auto`
* `n_jobs`：并行计算使用的 CPU 核数，默认为 `None`

属性：
* `embedding_`：降维后的数据矩阵
* `training_data`: 原始训练数据
* `dist_matrix_`: 邻居距离矩阵

## LocallyLinearEmbedding(LLE)

局部线性嵌入（LLE, Locally Linear Embedding）是一种基于流形学习的非线性降维方法。LLE 通过保持每个数据点及其邻居之间的局部线性关系，将高维数据嵌入到低维空间中。LLE 特别适用于数据在低维流形上有复杂非线性结构的情况, 其原型为：

```python
class sklearn.manifold.LocallyLinearEmbedding(n_neighbors=5, n_components=2, reg=1e-3, eigen_solver='auto', tol=1e-06, max_iter=100, method='standard', hessian_tol=1e-04, modified_tol=1e-12, neighbors_algorithm='auto', random_state=None)
```

* `n_neighbors`：指定每个数据点的邻居数量，默认为 `5`
* `n_components`：指定降维后的维度，默认为 `2`
* `reg`：正则化参数，默认为 `1e-3`
* `eigen_solver`：特征值求解方法，默认为 `auto`
* `tol`：停止迭代的阈值，默认为 `1e-06`
* `max_iter`：最大迭代次数，默认为 `100`
* `method`：嵌入方法
  * `standard`：标准 LLE
  * `hessian`：Hessian LLE
  * `modified`：修改后的 LLE
  * `ltsa`：局部切线空间对齐
* `hessian_tol`：Hessian 矩阵的停止迭代阈值，默认为 `1e-04`
* `modified_tol`：修改后的停止迭代阈值，默认为 `1e-12`
* `neighbors_algorithm`：邻居搜索算法
  * `auto`：自动选择
  * `ball_tree`：球树
  * `kd_tree`：k-d 树
  * `brute`：暴力搜索
* `random_state`：随机种子，默认为 `None`

属性：
* `embedding_vectors`：原始数据在低维空间的嵌入矩阵
* `reconstruction_error_`: 重构误差

## t-SNE

t-分布随机邻域嵌入（t-SNE, t-Distributed Stochastic Neighbor Embedding）是一种非线性降维技术，广泛用于数据可视化，特别是处理高维数据。t-SNE 旨在将高维数据映射到低维空间（通常是 2D 或 3D），以便能够直观地观察数据的结构和相似性, 其原型为：

```python
class sklearn.manifold.TSNE(n_components=2, perplexity=30.0, early_exaggeration=12.0, learning_rate='auto', n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5)
```

* `n_components`：指定降维后的维度，默认为 `2`
* `perplexity`：困惑度，默认为 `30.0`
* `early_exaggeration`:早期放大因子，默认为 `12.0`
* `learning_rate`：学习率，默认为 `auto`
* `n_iter`：最大迭代次数，默认为 `1000`
* `n_iter_without_progress`：在没有进度的情况下停止迭代的迭代次数，默认为 `300`
* `min_grad_norm`：最小梯度范数，默认为 `1e-07`
* `metric`：距离度量，默认为 `euclidean`
* `init`：初始化方法，默认为 `random`
* `verbose`：是否输出日志信息，默认为 `0`

属性：
* `embedding_`：降维后的数据矩阵
* `kl_divergence_`：Kullback-Leibler 散度
* `n_iter`: 实际迭代次数

## FA

因子分析（FA, Factor Analysis）是一种统计方法，用于数据降维和探索数据中潜在的隐含因素。因子分析的目标是通过较少的隐含因子来解释数据的方差和共变异性，从而减少特征数量并揭示数据的内在结构, 其原型为：

```python
class sklearn.decomposition.FactorAnalysis(n_components=None, tol=0.001, copy=True, max_iter=1000, noise_variance_init=None, svd_method='randomized', iterated_power=3, random_state=None)
```

* `n_components`：指定降维后的维度，默认为 `None`
* `tol`：停止迭代的阈值，默认为 `0.001`
* `copy`：是否复制数据，默认为 `True`
* `max_iter`：最大迭代次数，默认为 `1000`
* `noise_variance_init`：噪声方差初始化，默认为 `None`
* `svd_method`：SVD 方法，默认为 `randomized`
* `iterated_power`：迭代次数，默认为 `3`

属性：
* `components_`：因子载荷矩阵
* `noise_variance_`：噪声方差
* `loglike_`: 每次迭代的对数似然函数值
* `n_iter`: 迭代次数

## FastICA

快速独立成分分析（FastICA, Fast Independent Component Analysis）是一种用于盲源分离的非线性降维方法。它旨在从混合信号中提取出独立的源信号。在许多实际应用中，信号是由多个独立的信号混合而成的，FastICA 能够有效地分离这些独立的源信号, 其原型为:

```python
class sklearn.decomposition.FastICA(n_components=None, algorithm='parallel', whiten=True, fun='logcosh', fun_args=None, max_iter=200, tol=0.0001, w_init=None, random_state=None)
```

* `n_components`：指定降维后的维度，默认为 `None`
* `algorithm`：算法，默认为 `parallel`
* `whiten`：是否白化数据，默认为 `True`
* `fun`：非线性函数，默认为 `logcosh`
* `fun_args`：非线性函数的参数，默认为 `None`
* `max_iter`：最大迭代次数，默认为 `200`
* `tol`：停止迭代的阈值，默认为 `0.0001`
* `w_init`：初始化矩阵，默认为 `None`
* `random_state`：随机种子，默认为 `None`

属性：
* `components_`：独立成分矩阵
* `mixing_`：混合矩阵
* `n_iter_`: 实际迭代次数

## UMAP

统一流形逼近与投影（UMAP, Uniform Manifold Approximation and Projection）是一种用于降维和数据可视化的非线性方法。UMAP 旨在保留高维数据的局部和全局结构，通过将数据映射到低维空间来揭示其内在结构, 其原型为:

```python
class umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', metric_kwds=None, n_epochs=200, init='spectral', random_state=None, angular_rp_forest=False, target_weight=0.5, a=1.0, b=1.0, gamma=1.0, negative_sample_rate=5, transform_queue_size=4.0, transform_n_epochs=1000, transform_batch_size=1000, transform_mode='direct', force_all_finite=True, verbose=False)
```

* `n_neighbors`：指定邻域大小，默认为 `15`
* `min_dist`：最小距离，默认为 `0.1`
* `n_components`：指定降维后的维度，默认为 `2`
* `metric`：距离度量，默认为 `euclidean`
* `metric_kwds`：距离度量的参数，默认为 `None`
* `n_epochs`：最大迭代次数，默认为 `200`
* `init`：初始化方法，默认为 `spectral`
* `random_state`：随机种子，默认为 `None`

属性：
* `embedding_`：降维后的数据矩阵
* `n_epochs_`: 实际迭代次数
* `min_dist`: 最小距离
